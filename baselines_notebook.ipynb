{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.bow import *\n",
    "from utils import *\n",
    "from embeddings.glove import *\n",
    "from embeddings.tfidf import *\n",
    "from models.nn import *\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import pickle\n",
    "from models.rnn import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from preprocessing import *\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "from models.one_hot_vector import convert_to_one_hot_vec, get_features_ohv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = False\n",
    "\n",
    "tweets = []\n",
    "labels = []\n",
    "\n",
    "if (not full):\n",
    "    load_tweets(SMALL_TRAIN_POS, 0, tweets, labels)\n",
    "    load_tweets(SMALL_TRAIN_NEG, 1, tweets, labels)\n",
    "else:\n",
    "    load_tweets(TRAIN_POS, 0, tweets, labels)\n",
    "    load_tweets(TRAIN_NEG, 0, tweets, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = split_train_test(np.array(tweets), np.array(labels), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_pp, labels_pp = preprocess(tweets, labels)\n",
    "separator = \" \"  # Define the separator, which in this case is a space\n",
    "tweets_pp = [separator.join(tweet) for tweet in tweets_pp]\n",
    "X_train_pp, X_val_pp, Y_train_pp, Y_val_pp = split_train_test(np.array(tweets_pp), np.array(labels_pp), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['worries', 'fml', 'tooo', 'seeee', 'youuuu']\n",
      "Processed: ['worries', 'fuck my life', 'too', 'see', 'youu']\n",
      "\n",
      "Original: ['thiiis', 'is', 'aaamazing', 'and', 'coooool']\n",
      "Processed: ['thiis', 'is', 'aamazing', 'and', 'cool']\n",
      "\n",
      "Original: ['whaaaaat', 'a', 'beauuutiful', 'daaaay']\n",
      "Processed: ['whaat', 'a', 'beauutiful', 'daay']\n",
      "\n",
      "Original: ['heyy', 'theeerreeee', \"what's\", 'uuup']\n",
      "Processed: ['heyy', 'theerree', \"what's\", 'uup']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_tweets = [\n",
    "    [\"worries\", \"fml\", \"tooo\", \"seeee\", \"youuuu\"],\n",
    "    [\"thiiis\", \"is\", \"aaamazing\", \"and\", \"coooool\"],\n",
    "    [\"whaaaaat\", \"a\", \"beauuutiful\", \"daaaay\"],\n",
    "    [\"heyy\", \"theeerreeee\", \"what's\", \"uuup\"]\n",
    "]\n",
    "\n",
    "# Applying the function to the sample tweets\n",
    "processed_tweets = remove_repeated(sample_tweets)\n",
    "processed_tweets = expand_abbreviations(processed_tweets, ABBREVIATIONS)\n",
    "\n",
    "# Display the results\n",
    "for original, processed in zip(sample_tweets, processed_tweets):\n",
    "    print(\"Original:\", original)\n",
    "    print(\"Processed:\", processed)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Top 10 negative words\n",
      "yougetmajorpointsif -5.116773458519785\n",
      "bahaha -4.080151608532275\n",
      "smartnokialumia -3.5082983071390537\n",
      "waystomakemehappy -3.440534139678091\n",
      "worries -3.0610592153485476\n",
      "harrypotterchatuplines -2.9260008416897767\n",
      "thanx -2.6408653121258454\n",
      "therefore -2.5497920943340184\n",
      "ifindthatattractive -2.5286355540916654\n",
      "photographer -2.4957547617473956\n",
      "\n",
      "---- Top 10 positive words\n",
      "electronics 3.4836681586846003\n",
      "rip 3.4842811276775705\n",
      "apparel 3.700831391681254\n",
      "depressed 3.754107838340755\n",
      "misc 3.978292522735455\n",
      "depressing 4.076664533464381\n",
      "sadtweet 4.119527629840292\n",
      "saddest 5.333012560746026\n",
      "hardcover 7.501051086695297\n",
      "paperback 8.309226555870515\n",
      "\n",
      "ACCURACY: 0.802\n",
      "RECALL: 0.7660128102481986\n",
      "F1: 0.7944778908034047\n",
      "PRECISION: 0.8251401466149202\n",
      "Validation Accuracy: 0.802\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.84      0.81     10008\n",
      "           1       0.83      0.77      0.79      9992\n",
      "\n",
      "    accuracy                           0.80     20000\n",
      "   macro avg       0.80      0.80      0.80     20000\n",
      "weighted avg       0.80      0.80      0.80     20000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/selimjerad/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "bow_1 = bow(X_train, X_val, Y_train, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Top 10 negative words\n",
      "thankss -3.1107689449136267\n",
      "worries -2.5128494792527762\n",
      "blessing -2.4029983938400172\n",
      "nf -2.2614414333832924\n",
      "sweetest -2.1953173007267406\n",
      "ayee -2.141352392520665\n",
      "funn -2.1317873270763896\n",
      "appreciated -2.1188804172517264\n",
      "tuned -2.116599169521241\n",
      "pumped -2.1050817519357614\n",
      "\n",
      "---- Top 10 positive words\n",
      "dvd 3.184057734908375\n",
      "depressed 3.2678905108600227\n",
      "saddest 3.439264162061857\n",
      "guides 3.4657501730403544\n",
      "apparel 3.6387830502101517\n",
      "depressing 4.026632591006754\n",
      "electronics 4.15162325096423\n",
      "misc 4.841221858669179\n",
      "hardcover 8.281027014245268\n",
      "paperback 9.038994646479518\n",
      "\n",
      "ACCURACY: 0.7879874248524626\n",
      "RECALL: 0.7503558524033724\n",
      "F1: 0.780968660968661\n",
      "PRECISION: 0.8141855768088393\n",
      "Validation Accuracy: 0.7879874248524626\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.83      0.79      8998\n",
      "           1       0.81      0.75      0.78      9133\n",
      "\n",
      "    accuracy                           0.79     18131\n",
      "   macro avg       0.79      0.79      0.79     18131\n",
      "weighted avg       0.79      0.79      0.79     18131\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/selimjerad/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "bow_2 = bow(X_train_pp, X_val_pp, Y_train_pp, Y_val_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# One-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting training set: 100%|██████████| 180000/180000 [00:04<00:00, 40396.25it/s]\n",
      "Converting validation set: 100%|██████████| 20000/20000 [00:00<00:00, 39246.91it/s]\n",
      "C:\\Users\\Li Ting Luong\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Top 10 negative words\n",
      "#yougetmajorpointsif -4.737628233664645\n",
      "bahaha -4.3701856188649\n",
      "#smartnokialumia -3.850843746104472\n",
      "#waystomakemehappy -3.5979460560038636\n",
      "worries -3.147664480202091\n",
      "#harrypotterchatuplines -3.0716603696599694\n",
      "therefore -2.8550036454535865\n",
      "delicious -2.668267767752255\n",
      "thanx -2.514064617157143\n",
      "impress -2.514044352774007\n",
      "\n",
      "---- Top 10 positive words\n",
      "unfair 3.599135127718332\n",
      "wahhh 3.6176393927719346\n",
      "fml 3.661910284924043\n",
      "depressing 3.7873509153737426\n",
      "rip 3.8071883048871356\n",
      "#sadtweet 4.10726439302038\n",
      "hardcover 4.919052435466942\n",
      "(8 4.99767691141605\n",
      "saddest 5.665785725193116\n",
      "paperback 5.998566556266123\n",
      "\n",
      "ACCURACY: 0.8223\n",
      "RECALL: 0.7906325060048038\n",
      "F1: 0.8163687093107369\n",
      "PRECISION: 0.8438367870113224\n"
     ]
    }
   ],
   "source": [
    "# build vocabulary\n",
    "vocab = build_vocab(tweets)\n",
    "\n",
    "# we only keep the 5000 most frequent words, both to reduce the computational cost and reduce overfitting\n",
    "vectorizer = CountVectorizer(vocabulary=vocab)\n",
    "vectorizer.fit_transform(tweets)\n",
    "\n",
    "# convert tweets to one-hot vectors\n",
    "X_train_ohv = []\n",
    "X_val_ohv = []\n",
    "for tweet in tqdm(X_train, desc=\"Converting training set\"):\n",
    "    X_train_ohv.append(convert_to_one_hot_vec(tweet, vocab))\n",
    "for tweet in tqdm(X_val, desc=\"Converting validation set\"):\n",
    "    X_val_ohv.append(convert_to_one_hot_vec(tweet, vocab))\n",
    "\n",
    "# train logistic regression model\n",
    "model = LogisticRegression(C=1e5, max_iter=100)\n",
    "model.fit(X_train_ohv, Y_train)\n",
    "\n",
    "# print features\n",
    "get_features_ohv(model, vectorizer)\n",
    "\n",
    "# predict on validation set\n",
    "y_pred = model.predict(X_val_ohv)\n",
    "\n",
    "# print metrics\n",
    "get_basic_metrics(y_pred, Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_WIKI_200D = 'data/glove_wiki/glove.6B.200d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'GLOVE_WIKI_200D' from 'utils' (/Users/selimjerad/Desktop/whitewashed/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-4cd0e09ea6c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGLOVE_TWEET_100D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGLOVE_WIKI_100D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGLOVE_TWEET_200D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGLOVE_WIKI_200D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglove\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mall_tweets_to_glove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'GLOVE_WIKI_200D' from 'utils' (/Users/selimjerad/Desktop/whitewashed/utils.py)"
     ]
    }
   ],
   "source": [
    "from utils import GLOVE_TWEET_100D, GLOVE_WIKI_100D, GLOVE_TWEET_200D, GLOVE_WIKI_200D\n",
    "from embeddings.glove import all_tweets_to_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove_wiki = all_tweets_to_glove(X_train, GLOVE_WIKI_200D, 200)\n",
    "X_val_glove_wiki = all_tweets_to_glove(X_val, GLOVE_WIKI_200D, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove_tweet = all_tweets_to_glove(X_train, GLOVE_TWEET_200D, 200)\n",
    "X_val_glove_tweet = all_tweets_to_glove(X_val, GLOVE_TWEET_200D, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = build_vocab(tweets)\n",
    "X_train_tfidf, _ = transform_tweets_to_tfidf(X_train, vocabulary)\n",
    "X_val_tfidf, _ = transform_tweets_to_tfidf(X_val, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.66755\n",
      "RECALL: 0.6381104883907126\n",
      "F1: 0.6572857069223236\n",
      "PRECISION: 0.677649059411202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/selimjerad/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_glove_wiki, Y_train)\n",
    "get_basic_metrics(clf.predict(X_val_glove_wiki), Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.6795\n",
      "RECALL: 0.6531224979983987\n",
      "F1: 0.6706402219710205\n",
      "PRECISION: 0.6891235480464625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/selimjerad/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_glove_tweet, Y_train)\n",
    "get_basic_metrics(clf.predict(X_val_glove_tweet), Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.80015\n",
      "RECALL: 0.7748198558847078\n",
      "F1: 0.7948257276320517\n",
      "PRECISION: 0.8158920855727685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/selimjerad/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_tfidf, Y_train)\n",
    "get_basic_metrics(clf.predict(X_val_tfidf), Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.nn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe + NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = all_tweets_to_glove(X_train,  GLOVE_TWEET_200D, 200)\n",
    "X_val = all_tweets_to_glove(X_val, GLOVE_TWEET_200D, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.tensor(X_train).to(torch.float32), torch.tensor(Y_train).to(torch.float32))\n",
    "test_dataset = TensorDataset(torch.tensor(X_val).to(torch.float32), torch.tensor(Y_val).to(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model = train_simple_nn(train_loader, 200)\n",
    "test_simple_nn(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF + NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dense = torch.tensor(X_train_tfidf.toarray(), dtype=torch.float32)\n",
    "X_val_dense = torch.tensor(X_val_tfidf.toarray(), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_dense, torch.tensor(Y_train).to(torch.float32))\n",
    "test_dataset = TensorDataset(X_val_dense, torch.tensor(Y_val).to(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF + NN\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model = train_simple_nn(train_loader, 5000)\n",
    "test_simple_nn(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train, tokens_val = get_tokens_rnn(X_train, X_val)\n",
    "train_dataset = TensorDataset(torch.tensor(tokens_train, dtype=torch.long),\n",
    "                              torch.from_numpy(Y_train.astype(np.float32)))\n",
    "test_dataset = TensorDataset(torch.tensor(tokens_val, dtype=torch.long), torch.from_numpy(Y_val.astype(np.float32)))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.40875697135925293\n",
      "Epoch 2/20, Loss: 0.4908829927444458\n",
      "Epoch 3/20, Loss: 0.4381594955921173\n",
      "Epoch 4/20, Loss: 0.42911550402641296\n",
      "Epoch 5/20, Loss: 0.4773136079311371\n",
      "Epoch 6/20, Loss: 0.443154901266098\n",
      "Epoch 7/20, Loss: 0.2574199140071869\n",
      "Epoch 8/20, Loss: 0.418632447719574\n",
      "Epoch 9/20, Loss: 0.29267454147338867\n",
      "Epoch 10/20, Loss: 0.32499146461486816\n",
      "Epoch 11/20, Loss: 0.4105643332004547\n",
      "Epoch 12/20, Loss: 0.32943469285964966\n",
      "Epoch 13/20, Loss: 0.43922320008277893\n",
      "Epoch 14/20, Loss: 0.36273807287216187\n",
      "Epoch 15/20, Loss: 0.2680851221084595\n",
      "Epoch 16/20, Loss: 0.5699405074119568\n",
      "Epoch 17/20, Loss: 0.30537500977516174\n",
      "Epoch 18/20, Loss: 0.28328511118888855\n",
      "Epoch 19/20, Loss: 0.313154011964798\n",
      "Epoch 20/20, Loss: 0.325120210647583\n",
      "Validation Accuracy: 0.80745\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.83      0.81     10008\n",
      "         1.0       0.82      0.78      0.80      9992\n",
      "\n",
      "    accuracy                           0.81     20000\n",
      "   macro avg       0.81      0.81      0.81     20000\n",
      "weighted avg       0.81      0.81      0.81     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = train_rnn(train_loader)\n",
    "test_rnn(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Size mismatch between tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tokens_train, tokens_val \u001b[38;5;241m=\u001b[39m get_tokens_rnn(X_train_pp, X_val_pp)\n\u001b[0;32m----> 2\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTensorDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(torch\u001b[38;5;241m.\u001b[39mtensor(tokens_val, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong), torch\u001b[38;5;241m.\u001b[39mfrom_numpy(Y_val\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)))    \n\u001b[1;32m      5\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cil/lib/python3.8/site-packages/torch/utils/data/dataset.py:202\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mtensors: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(tensors[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m tensors), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize mismatch between tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors\n",
      "\u001b[0;31mAssertionError\u001b[0m: Size mismatch between tensors"
     ]
    }
   ],
   "source": [
    "tokens_train, tokens_val = get_tokens_rnn(X_train_pp, X_val_pp)\n",
    "train_dataset = TensorDataset(torch.tensor(tokens_train, dtype=torch.long),\n",
    "                              torch.from_numpy(Y_train_pp.astype(np.float32)))\n",
    "test_dataset = TensorDataset(torch.tensor(tokens_val, dtype=torch.long), torch.from_numpy(Y_val_pp.astype(np.float32)))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_rnn(train_loader)\n",
    "test_rnn(test_loader, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
