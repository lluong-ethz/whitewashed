{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "from models.bow import *\n",
    "from utils import *\n",
    "from embeddings.glove import *\n",
    "from embeddings.tfidf import *\n",
    "from models.nn import *\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from models.rnn import *\n",
    "from preprocessing import *\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change `full = False` to `full = True` to use the entire dataset (depending on the memory capacities available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = False\n",
    "\n",
    "tweets = []\n",
    "labels = []\n",
    "\n",
    "if (not full):\n",
    "    load_tweets(SMALL_TRAIN_POS, 0, tweets, labels)\n",
    "    load_tweets(SMALL_TRAIN_NEG, 1, tweets, labels)\n",
    "else:\n",
    "    load_tweets(TRAIN_POS, 0, tweets, labels)\n",
    "    load_tweets(TRAIN_NEG, 0, tweets, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset non pre-processed\n",
    "X_train, X_val, Y_train, Y_val = split_train_test(np.array(tweets), np.array(labels), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can adjust the method `preprocess` in `preprocessing.py` to change which pre-preprocessing methods are being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset pre-processed\n",
    "tweets_pp, labels_pp = preprocess(tweets, labels)\n",
    "separator = \" \"  # Define the separator, which in this case is a space\n",
    "tweets_pp = [separator.join(tweet) for tweet in tweets_pp]\n",
    "X_train_pp, X_val_pp, Y_train_pp, Y_val_pp = split_train_test(np.array(tweets_pp), np.array(labels_pp), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['worries', 'fml', 'tooo', 'seeee', 'youuuu']\n",
      "Processed: ['worries', 'fuck my life', 'too', 'see', 'youu']\n",
      "\n",
      "Original: ['thiiis', 'is', 'aaamazing', 'and', 'coooool']\n",
      "Processed: ['thiis', 'is', 'aamazing', 'and', 'cool']\n",
      "\n",
      "Original: ['whaaaaat', 'a', 'beauuutiful', 'daaaay']\n",
      "Processed: ['whaat', 'a', 'beauutiful', 'daay']\n",
      "\n",
      "Original: ['heyy', 'theeerreeee', \"what's\", 'uuup']\n",
      "Processed: ['heyy', 'theerree', \"what's\", 'uup']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test codes to see the effects of pre-processing\n",
    "sample_tweets = [\n",
    "    [\"worries\", \"fml\", \"tooo\", \"seeee\", \"youuuu\"],\n",
    "    [\"thiiis\", \"is\", \"aaamazing\", \"and\", \"coooool\"],\n",
    "    [\"whaaaaat\", \"a\", \"beauuutiful\", \"daaaay\"],\n",
    "    [\"heyy\", \"theeerreeee\", \"what's\", \"uuup\"]\n",
    "]\n",
    "\n",
    "# Applying the function to the sample tweets\n",
    "processed_tweets = remove_repeated(sample_tweets)\n",
    "processed_tweets = expand_abbreviations(processed_tweets, ABBREVIATIONS)\n",
    "\n",
    "# Display the results\n",
    "for original, processed in zip(sample_tweets, processed_tweets):\n",
    "    print(\"Original:\", original)\n",
    "    print(\"Processed:\", processed)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Top 10 negative words\n",
      "yougetmajorpointsif -5.116773458519785\n",
      "bahaha -4.080151608532275\n",
      "smartnokialumia -3.5082983071390537\n",
      "waystomakemehappy -3.440534139678091\n",
      "worries -3.0610592153485476\n",
      "harrypotterchatuplines -2.9260008416897767\n",
      "thanx -2.6408653121258454\n",
      "therefore -2.5497920943340184\n",
      "ifindthatattractive -2.5286355540916654\n",
      "photographer -2.4957547617473956\n",
      "\n",
      "---- Top 10 positive words\n",
      "electronics 3.4836681586846003\n",
      "rip 3.4842811276775705\n",
      "apparel 3.700831391681254\n",
      "depressed 3.754107838340755\n",
      "misc 3.978292522735455\n",
      "depressing 4.076664533464381\n",
      "sadtweet 4.119527629840292\n",
      "saddest 5.333012560746026\n",
      "hardcover 7.501051086695297\n",
      "paperback 8.309226555870515\n",
      "\n",
      "ACCURACY: 0.802\n",
      "RECALL: 0.7660128102481986\n",
      "F1: 0.7944778908034047\n",
      "PRECISION: 0.8251401466149202\n",
      "Validation Accuracy: 0.802\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.84      0.81     10008\n",
      "           1       0.83      0.77      0.79      9992\n",
      "\n",
      "    accuracy                           0.80     20000\n",
      "   macro avg       0.80      0.80      0.80     20000\n",
      "weighted avg       0.80      0.80      0.80     20000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/selimjerad/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "bow_1 = bow(X_train, X_val, Y_train, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Top 10 negative words\n",
      "thankss -3.1107689449136267\n",
      "worries -2.5128494792527762\n",
      "blessing -2.4029983938400172\n",
      "nf -2.2614414333832924\n",
      "sweetest -2.1953173007267406\n",
      "ayee -2.141352392520665\n",
      "funn -2.1317873270763896\n",
      "appreciated -2.1188804172517264\n",
      "tuned -2.116599169521241\n",
      "pumped -2.1050817519357614\n",
      "\n",
      "---- Top 10 positive words\n",
      "dvd 3.184057734908375\n",
      "depressed 3.2678905108600227\n",
      "saddest 3.439264162061857\n",
      "guides 3.4657501730403544\n",
      "apparel 3.6387830502101517\n",
      "depressing 4.026632591006754\n",
      "electronics 4.15162325096423\n",
      "misc 4.841221858669179\n",
      "hardcover 8.281027014245268\n",
      "paperback 9.038994646479518\n",
      "\n",
      "ACCURACY: 0.7879874248524626\n",
      "RECALL: 0.7503558524033724\n",
      "F1: 0.780968660968661\n",
      "PRECISION: 0.8141855768088393\n",
      "Validation Accuracy: 0.7879874248524626\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.83      0.79      8998\n",
      "           1       0.81      0.75      0.78      9133\n",
      "\n",
      "    accuracy                           0.79     18131\n",
      "   macro avg       0.79      0.79      0.79     18131\n",
      "weighted avg       0.79      0.79      0.79     18131\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/selimjerad/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "bow_2 = bow(X_train_pp, X_val_pp, Y_train_pp, Y_val_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import GLOVE_TWEET_100D, GLOVE_WIKI_100D, GLOVE_TWEET_200D, GLOVE_WIKI_200D\n",
    "from embeddings.glove import all_tweets_to_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove_wiki = all_tweets_to_glove(X_train, GLOVE_WIKI_200D, 200)\n",
    "X_val_glove_wiki = all_tweets_to_glove(X_val, GLOVE_WIKI_200D, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove_tweet = all_tweets_to_glove(X_train, GLOVE_TWEET_200D, 200)\n",
    "X_val_glove_tweet = all_tweets_to_glove(X_val, GLOVE_TWEET_200D, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = build_vocab(tweets)\n",
    "X_train_tfidf, _ = transform_tweets_to_tfidf(X_train, vocabulary)\n",
    "X_val_tfidf, _ = transform_tweets_to_tfidf(X_val, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.66755\n",
      "RECALL: 0.6381104883907126\n",
      "F1: 0.6572857069223236\n",
      "PRECISION: 0.677649059411202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/selimjerad/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_glove_wiki, Y_train)\n",
    "get_basic_metrics(clf.predict(X_val_glove_wiki), Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.6795\n",
      "RECALL: 0.6531224979983987\n",
      "F1: 0.6706402219710205\n",
      "PRECISION: 0.6891235480464625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/selimjerad/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_glove_tweet, Y_train)\n",
    "get_basic_metrics(clf.predict(X_val_glove_tweet), Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.80015\n",
      "RECALL: 0.7748198558847078\n",
      "F1: 0.7948257276320517\n",
      "PRECISION: 0.8158920855727685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/selimjerad/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_tfidf, Y_train)\n",
    "get_basic_metrics(clf.predict(X_val_tfidf), Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF + Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.75605\n",
      "RECALL: 0.6637309847878302\n",
      "F1: 0.7310808576310421\n",
      "PRECISION: 0.8136424978530241\n",
      "ACCURACY: 0.75605\n",
      "RECALL: 0.6637309847878302\n",
      "F1: 0.7310808576310421\n",
      "PRECISION: 0.8136424978530241\n"
     ]
    }
   ],
   "source": [
    "from models.naivebayes import *\n",
    "\n",
    "# build vocabulary\n",
    "vocab = build_vocab(tweets)\n",
    "\n",
    "# transform the tweets into TF-IDF features\n",
    "X, vectorizer = transform_tweets_to_tfidf(tweets, vocab)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = split_train_test(np.array(X), np.array(labels), 1)\n",
    "\n",
    "# train Naïve Bayes model\n",
    "model = train_naive_bayes(X_train,Y_train)\n",
    "\n",
    "# make predictions with Naïve Bayes\n",
    "y_pred = predict_naive_bayes(model, X_val)\n",
    "\n",
    "# print metrics\n",
    "get_basic_metrics(y_pred, Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Neural Networks (NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.nn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe + NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove_tweet = all_tweets_to_glove(X_train,  GLOVE_TWEET_200D, 200)\n",
    "X_val_glove_tweet = all_tweets_to_glove(X_val, GLOVE_TWEET_200D, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.tensor(X_train_glove_tweet).to(torch.float32), torch.tensor(Y_train).to(torch.float32))\n",
    "test_dataset = TensorDataset(torch.tensor(X_val_glove_tweet).to(torch.float32), torch.tensor(Y_val).to(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model = train_simple_nn(train_loader, 200)\n",
    "test_simple_nn(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF + NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = build_vocab(tweets)\n",
    "X_train_tfidf, _ = transform_tweets_to_tfidf(X_train, vocabulary)\n",
    "X_val_tfidf, _ = transform_tweets_to_tfidf(X_val, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dense = torch.tensor(X_train_tfidf.toarray(), dtype=torch.float32)\n",
    "X_val_dense = torch.tensor(X_val_tfidf.toarray(), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_dense, torch.tensor(Y_train).to(torch.float32))\n",
    "test_dataset = TensorDataset(X_val_dense, torch.tensor(Y_val).to(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model = train_simple_nn(train_loader, 5000)\n",
    "test_simple_nn(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train, tokens_val = get_tokens_rnn(X_train, X_val)\n",
    "train_dataset = TensorDataset(torch.tensor(tokens_train, dtype=torch.long),\n",
    "                              torch.from_numpy(Y_train.astype(np.float32)))\n",
    "test_dataset = TensorDataset(torch.tensor(tokens_val, dtype=torch.long), torch.from_numpy(Y_val.astype(np.float32)))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.40875697135925293\n",
      "Epoch 2/20, Loss: 0.4908829927444458\n",
      "Epoch 3/20, Loss: 0.4381594955921173\n",
      "Epoch 4/20, Loss: 0.42911550402641296\n",
      "Epoch 5/20, Loss: 0.4773136079311371\n",
      "Epoch 6/20, Loss: 0.443154901266098\n",
      "Epoch 7/20, Loss: 0.2574199140071869\n",
      "Epoch 8/20, Loss: 0.418632447719574\n",
      "Epoch 9/20, Loss: 0.29267454147338867\n",
      "Epoch 10/20, Loss: 0.32499146461486816\n",
      "Epoch 11/20, Loss: 0.4105643332004547\n",
      "Epoch 12/20, Loss: 0.32943469285964966\n",
      "Epoch 13/20, Loss: 0.43922320008277893\n",
      "Epoch 14/20, Loss: 0.36273807287216187\n",
      "Epoch 15/20, Loss: 0.2680851221084595\n",
      "Epoch 16/20, Loss: 0.5699405074119568\n",
      "Epoch 17/20, Loss: 0.30537500977516174\n",
      "Epoch 18/20, Loss: 0.28328511118888855\n",
      "Epoch 19/20, Loss: 0.313154011964798\n",
      "Epoch 20/20, Loss: 0.325120210647583\n",
      "Validation Accuracy: 0.80745\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.83      0.81     10008\n",
      "         1.0       0.82      0.78      0.80      9992\n",
      "\n",
      "    accuracy                           0.81     20000\n",
      "   macro avg       0.81      0.81      0.81     20000\n",
      "weighted avg       0.81      0.81      0.81     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = train_rnn(train_loader)\n",
    "test_rnn(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train, tokens_val = get_tokens_rnn(X_train_pp, X_val_pp)\n",
    "train_dataset = TensorDataset(torch.tensor(tokens_train, dtype=torch.long),\n",
    "                              torch.from_numpy(Y_train_pp.astype(np.float32)))\n",
    "test_dataset = TensorDataset(torch.tensor(tokens_val, dtype=torch.long), torch.from_numpy(Y_val_pp.astype(np.float32)))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.30951792001724243\n",
      "Epoch 2/20, Loss: 0.5805522203445435\n",
      "Epoch 3/20, Loss: 0.1933625489473343\n",
      "Epoch 4/20, Loss: 0.2470891922712326\n",
      "Epoch 5/20, Loss: 0.5216470956802368\n",
      "Epoch 6/20, Loss: 0.4219811260700226\n",
      "Epoch 7/20, Loss: 0.27058297395706177\n",
      "Epoch 8/20, Loss: 0.3670650124549866\n",
      "Epoch 9/20, Loss: 0.37213075160980225\n",
      "Epoch 10/20, Loss: 0.3277910649776459\n",
      "Epoch 11/20, Loss: 0.23644530773162842\n",
      "Epoch 12/20, Loss: 0.4055480360984802\n",
      "Epoch 13/20, Loss: 0.34326204657554626\n",
      "Epoch 14/20, Loss: 0.8441938757896423\n",
      "Epoch 15/20, Loss: 0.2527165710926056\n",
      "Epoch 16/20, Loss: 0.5450261235237122\n",
      "Epoch 17/20, Loss: 0.26861846446990967\n",
      "Epoch 18/20, Loss: 0.18486329913139343\n",
      "Epoch 19/20, Loss: 0.18546807765960693\n",
      "Epoch 20/20, Loss: 0.2423204928636551\n",
      "Validation Accuracy: 0.7908554409574762\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.80      0.79      8998\n",
      "         1.0       0.80      0.78      0.79      9133\n",
      "\n",
      "    accuracy                           0.79     18131\n",
      "   macro avg       0.79      0.79      0.79     18131\n",
      "weighted avg       0.79      0.79      0.79     18131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = train_rnn(train_loader)\n",
    "test_rnn(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Li Ting Luong\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4500/4500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m338s\u001b[0m 74ms/step - accuracy: 0.7776 - loss: 0.4516 - val_accuracy: 0.8292 - val_loss: 0.3680\n",
      "Epoch 2/2\n",
      "\u001b[1m4500/4500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 74ms/step - accuracy: 0.8711 - loss: 0.2913 - val_accuracy: 0.8277 - val_loss: 0.3802\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step\n",
      "ACCURACY: 0.82715\n",
      "RECALL: 0.811048839071257\n",
      "F1: 0.8242054411390796\n",
      "PRECISION: 0.837795926806575\n"
     ]
    }
   ],
   "source": [
    "from models.bilstm import *\n",
    "\n",
    "# tokenize and pad tweets & get vocabulary with tokenized tweets\n",
    "X, vocab_size = prepare_tweets(tweets)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = split_train_test(np.array(X), np.array(labels), 1)\n",
    "\n",
    "input_length = X.shape[1]\n",
    "embedding_dim = 200\n",
    "\n",
    "# create Bidirectional LSTM model\n",
    "bilstm_model = create_bilstm_model(vocab_size, embedding_dim, input_length)\n",
    "\n",
    "# train model\n",
    "model = train_bilstm(bilstm_model, X_train, Y_train)\n",
    "\n",
    "# predict on validation set\n",
    "y_pred = predict_bilstm(model, X_val)\n",
    "\n",
    "# print metrics\n",
    "get_basic_metrics(y_pred, Y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
